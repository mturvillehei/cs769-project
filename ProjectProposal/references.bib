@misc{hu2021lora,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2106.09685}, 
}

@misc{liu2024dora,
	title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
	author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
	year={2024},
	eprint={2402.09353},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.09353}, 
}

@misc{dettmers2023qlora,
	title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
	author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
	year={2023},
	eprint={2305.14314},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2305.14314}, 
}

@misc{he2022unified,
	title={Towards a Unified View of Parameter-Efficient Transfer Learning}, 
	author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
	year={2022},
	eprint={2110.04366},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2110.04366}, 
}

@InProceedings{li2024instruction,
	author="Li, Zou
	and Pang, Ning
	and Zhao, Xiang",
	editor="Jin, Cheqing
	and Yang, Shiyu
	and Shang, Xuequn
	and Wang, Haofen
	and Zhang, Yong",
	title="Instruction Tuning Large Language Models forÂ Multimodal Relation Extraction Using LoRA",
	booktitle="Web Information Systems and Applications",
	year="2024",
	publisher="Springer Nature Singapore",
	address="Singapore",
	pages="364--376",
	abstract="The rapid proliferation of multimodal data on social platforms, particularly text-image pairs, has necessitated advanced methodologies for Multimodal Relation Extraction (MRE) to accurately identify semantic relations between annotated entities. Traditional approaches to MRE often suffer from limited generalizability across different datasets. To address the challenge, we propose to Instruction-Tune Large Language Models for MRE (ITMRE), which further utilizes Low-Rank Adaptation (LoRA) to tailor the Multimodal Large Language Models (MLLMs) for MRE. Our approach simplifies the extraction process using a two-stage multiple-choice question-answer template to first identify the types of two annotated entities in the text and then infer their relations. This structured methodology, combined with targeted LoRA tuning, allows for efficient model optimization without extensive retraining. We demonstrate the effectiveness of ITMRE through comprehensive experiments on the MNRE dataset, where it significantly outperforms existing methods and leading-edge language models such as GPT-3.5 and GPT-4 in terms of precision, recall, and F1 score, with an improvement in F1 scores by over 8{\%}.",
	isbn="978-981-97-7707-5"
}

@inproceedings{novikova2017e2e,
	title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
	author = "Novikova, Jekaterina  and
	Du{\v{s}}ek, Ond{\v{r}}ej  and
	Rieser, Verena",
	editor = "Jokinen, Kristiina  and
	Stede, Manfred  and
	DeVault, David  and
	Louis, Annie",
	booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
	month = aug,
	year = "2017",
	address = {Saarbr{\"u}cken, Germany},
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W17-5525",
	doi = "10.18653/v1/W17-5525",
	pages = "201--206",
	abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}