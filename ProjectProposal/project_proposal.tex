% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{graphicx}


\title{Exploring Methods for Efficient Fine-tuning of LLMs for Semantic Evaluation}
\author{
	Nicholas Boddy \and Caleb Raschka \and Morgan Turville-Heitz \\
	\textit{University of Wisconsin-Madison}
}
\date{\today}


\begin{document}
\maketitle

\begin{center}
	\textit{Project Proposal for CS 769 - Advanced Natural Language Processing}
\end{center}


\section{Project Overview}

LoRA is a powerful method for the fine-tuning of LLMs (and large models in general) on lower end hardware. As students, engaging with modern machine learning research can be challenging; many of us are limited in computational resources to at best a single commercial CUDA-compatible GPU. For this reason, efficient fine-tuning is exceptionally enticing. We therefore propose to first re-implement LoRA as the first part of our project, with further goals of exploring other fine-tuning methods like QLoRA. We intend to use this fine-tuning work as the foundation for our final project goal of efficient semantic evaluation, and for the initial work (Homework 3) we have reimplemented the fine-tuning of GPT2-Medium via LoRA as shown in Hu et al. (2021). This preliminary work shows the capabilities of LoRA for the fine-tuning of models on at-home hardware, which is a typical limitation for LLMs. 

We propose that, with either LoRA or QLoRA implementation, we will be able to meet baseline performance on the F1-Macro and F1-Micro metrics for the SEMEVAL 2021 Task 6\footnote{Task: propaganda.math.unipd.it/semeval2021task6/}. SEMEVAL 2021 task 6, titled “Detection of Persuasive Techniques in Texts and Images,” concerns the semantic identification of forms of propagandistic narratives used in memes. In particular, we are interested in which, if any, of 20 forms of narrative technique is utilized in the meme in order to convey an agenda. We will be approaching subtask 1 in particular, because subtask 2 and subtask 3 will require more computing resources than we have access to.  

In order to achieve these goals, we will be working with three LLM models with sizes appropriate for our at-home resources. We will be looking at mid-sized LLaMa 7B, 13B, GPT2-medium and GPT2-large. These will be fine-tuned off of the SEMEVAL 2021 Task 6 datasets for subtask 1\footnote{Data: github.com/di-dimitrov/SEMEVAL-2021-task6-corpus/tree/main/data}, with the end-goal of predicting the form of propaganda technique in the original meme. Our results for each model will be compared with the other fine tuned models, and will also be compared against the task leaderboard. We expect that the implementation of QLoRA will further increase the capacity for model sizes that we are able to fine tune; if the implementation of QLoRA is successful, we will fine tune even larger models for comparison.

In addition to comparing accuracies of these models against those on the existing leaderboards, we plan to compare both training times and inference times between the models to judge how effectively efficient the various fine-tuning methods are.


\section{Related Literature}

The efficient adaptation of large language models (LLMs) has become increasingly important as models grow in size and computational requirements. Hu et al. (2021) introduced LoRA (Low-Rank Adaptation), a foundational method that significantly reduces the memory and computational costs of fine-tuning LLMs. LoRA works by freezing the pretrained model weights and injecting trainable rank decomposition matrices into each layer of the transformer architecture. This approach reduces the number of trainable parameters by several orders of magnitude while maintaining model quality comparable to full fine-tuning. The key insight of LoRA is that the changes in weights during model adaptation have a low "intrinsic rank," allowing for effective parameter-efficient fine-tuning.
Building on Hu et al.’s prior work, He et al. (2021) provided a unified framework for parameter-efficient tuning methods. Through analysis of adapters, prefix tuning, and LoRA, they showed these methods still lagged behind full fine-tuning on challenging tasks like summarization and machine translation. By demonstrating that prefix tuning could be reformulated as a form of adapter, they identified key design dimensions across methods. Their experiments on diverse benchmarks led to the Mix-and-Match (MAM) Adapter, which matched full fine-tuning performance while using only 6.7\% of trainable parameters on generation tasks and 0.5\% on classification tasks.
Further advancements on the low rank adaptation methodology were made by Dettmers et al. (2023), advancing the field further last year with QLoRA, which combines quantization with LoRA to achieve even greater memory efficiency. QLoRA introduces several innovations, including 4-bit NormalFloat quantization (optimized for normally distributed weights), double quantization (which quantizes the quantization constants), and paged optimizers to manage memory spikes. This combination enables the fine-tuning of large models (up to 65B parameters) on a single GPU while maintaining performance comparable to full 16-bit fine-tuning. 
This year, Liu et al. (2024) proposed DoRA (Weight-Decomposed Low-Rank Adaptation), which introduces a weight decomposition analysis to investigate the inherent differences between full fine-tuning and LoRA. DoRA decomposes pretrained weights into magnitude and directional components, using LoRA specifically for directional updates. This decomposition approach enhances both learning capacity and training stability compared to standard LoRA, while maintaining the advantage of no additional inference overhead. Their analysis revealed that DoRA exhibits learning patterns more similar to full fine-tuning than LoRA does, leading to improved performance across various tasks and model architectures.


\section{Preliminary Implementation}

We reimplement the LoRA paper and test on GPT 2 - Medium for robustness in our solution. The repository for our implementation can be found here: mturvillehei/cs769-project: Team ID - 7.
LoRA layers are applied to the query and value matrices of attention heads, as discussed in LoRA for LLMs (Hu et al., 2021). This choice appears to be made due in order to limit the experiment from becoming too costly. This sentiment may be corroborated by the QLoRA paper (Dettmers et al., 2023), which states that to get full fine-tuning level performance, they needed to apply LoRA layers to the linear layers of the network as well. When tracking the memory cost of loss.backward() and optimizer.step() on GPT-2, we found that GPU memory was considerably reduced by using LoRA layers, about 70\% of the memory used on full fine-tuning. We experimented on the same dataset used in the paper, the E2E-NLG dataset (Novikova et al., 2017). With less compute, we could not use as robust of hyperparameters for training, so we opt to train our own baseline full fine-tuned GPT 2 - Medium to compare against.

LoRA layers caused a slight performance dip, but learning was still considerable across both models. Our learnable parameter reduction is on par with the original implementation as well, with a nearly 1000x smaller learnable space. Each query and value matrix is computed using a parameter space of 1024x1024. With LoRA layers using rank = 4, we can tune these matrices using 1024x4 + 4x1024 = 8192 parameters vs the original 1M. With 24 attention blocks in GPT 2, and 16384 LoRA parameters inside of an attention block, we arrive at the above number of around 390K parameters. Because GPT-2 has a unified attention layer that computes K,V,Q in one dense Conv1D layer, we need to inject LoRA into thi, break up it’s output, and add our LoRA output to the corresponding chunks before concatenating the unified K,V,Q matrix back together and passing it forward.


\section{Potential Work}

Building on the existing fine-tuning approaches of LoRA and QLoRA, we propose several methodological extensions specifically tailored for semantic evaluation tasks. Our proposed methods focus on addressing three key challenges: context preservation, memory efficiency, and adaption to multi-label classification scenarios. In order to extend our work beyond the existing LoRA and QLoRA work, we will be using LoRA and QLoRA training methods for the classification tasks required by SEMEVAL. 

In order to vary our research methods beyond the existing LoRA and QLoRA work, we will experiment with how we apply LoRA to the attention matrices and the linear layers. Some examples of variations are applying LoRA updates only to the key and value matrices, only applying to attention heads (rather than the linear layer as well), and more. We will also experiment with the floating point values that we settle on, and the learning rates compared to the original paper. 

Another potential method we can explore is the implementation of a hierarchical extension to the LoRA architecture that specifically targets semantic classification tasks. In particular, instead of applying rank decomposition uniformly across all transformer layers, we introduce a layer-specific ranking scheme where:
\begin{enumerate}
	\item Lower layers maintain higher rank matrices to capture fundamental semantic features
	\item Middle layers utilize dynamic rank allocation based on attention patterns
	\item Upper layers employ lower rank decomposition to focus on task-specific classification
\end{enumerate}

This hierarchical approach can potentially capture semantic information, and remain within the high-efficiency regime of LoRA training.


\section{Experiment Details}

At the top level, we will be exploring the comparisons between different efficient fine-tuning methods (primarily LoRA and QLoRA), different models (LLaMa, GPT2, etc.), and the two metrics for the classification tasks (F1-macro and F1-micro). In particular, this will require a standardized test of the training parameters during fine-tuning between models, a comparison of the different metrics that we will be comparing, and a quantitative analysis of how each model performs on the dataset for predicting propagandistic techniques.
Initially, this will require implementation of both the LoRA and QLoRA methods for the chosen task, and involves the development of the classifier. Once we have developed the codebase for the LoRA and QLoRA fine-tuning, we will begin optimizing the experimental parameters. In particular, we will be running hyperparameter optimization to identify the best settings for each model. This will require repeated fine-tuning attempts on our home machines, and we will show these results in the final report.
For our experimental setup, we will utilize the SEMEVAL 2021 Task 6 dataset, which contains approximately 950 memes annotated with 20 different propaganda techniques. We will split this dataset into standard 80-10-10 training, validation, and test sets while maintaining class distribution. Given the inherent class imbalance in propaganda techniques, we will implement weighted sampling during training to ensure adequate representation of less common techniques. Our training process will involve systematic evaluation of key hyperparameters including learning rate (ranging from 1e-5 to 1e-3), LoRA rank (4, 8, 16, 32), and alpha scaling factors (4, 8, 16, 32).
To ensure reproducibility and fair comparison between models, we will standardize our training environment across all experiments. This includes fixing the random seeds, utilizing the same batch size where possible (adjusted based on VRAM constraints), and maintaining consistent evaluation intervals. For each model configuration, we will conduct three training runs with different random seeds to account for training variance. Our evaluation metrics will focus primarily on F1-macro and F1-micro scores, but we will also track training time, memory usage, and convergence rates to provide a comprehensive analysis of the efficiency-performance trade-off.


\section{Logistics and Planning}

Caleb has implemented the LoRA code for this submission (Homework 3), specifically for the fine-tuning of GPT2-Medium on the original data from Hu et al. (2021). Moving forward, each of us will be responsible for fine-tuning of individual models on our home computers. In particular, this will be dependent on the availability of VRAM for each of us. Caleb has access to the largest amount of VRAM, and will handle training the larger models. Nick and Morgan will work on the slightly smaller mid-sized models.
For the development of the code for hyperparameter optimization, as well as the code base for actually running the fine-tuning and evaluation of the model, this will be collaborative, and we will be working on this jointly.
Our timeline will be structured in three main phases. In Phase 1 (weeks 1-2), we will focus on implementing and validating the QLoRA methodology, with Caleb leading the implementation while Nick and Morgan develop the evaluation framework and metrics tracking system. Phase 2 (weeks 3-4) will involve parallel training runs across our distributed hardware, with each team member responsible for specific model configurations. We will maintain a shared results repository and conduct weekly sync meetings to discuss progress and challenges. The final phase (weeks 5-6) will be dedicated to comprehensive analysis and report writing. This will be a collaborative effort as well.

\cite{dettmers2023qlora}
\cite{he2022unified}
\cite{hu2021lora}
\cite{li2024instruction}
\cite{liu2024dora}
\cite{novikova2017e2e}

\bibliographystyle{acl_natbib}
\bibliography{references}

\end{document}
