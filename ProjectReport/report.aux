\relax 
\bibstyle{acl_natbib}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hu2021lora}
\citation{dettmers2023qlora}
\citation{hu2021lora}
\citation{hu2021lora}
\citation{hu2021lora}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Literature}{1}{section.2}\protected@file@percent }
\citation{he2022unified}
\citation{he2022unified}
\citation{dettmers2023qlora}
\citation{dettmers2023qlora}
\citation{dettmers2023qlora}
\citation{liu2024dora}
\citation{novikova2017e2e}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces LoRA exhibits both better scalability and task performance in comparison to alternative parameter-efficient fine-tuning methods. Additionally, task performance is measured to meet, if not exceed that of full fine-tuning. \citep  {hu2021lora}}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ft_scale}{{1}{2}{LoRA exhibits both better scalability and task performance in comparison to alternative parameter-efficient fine-tuning methods. Additionally, task performance is measured to meet, if not exceed that of full fine-tuning. \cite {hu2021lora}}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}\protected@file@percent }
\citation{hu2021lora}
\citation{liu2024dora}
\citation{dettmers2023qlora}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Difference in finetuning methods. QLoRA supplements LoRA by applying quantization to the transformer model. \citep  {dettmers2023qlora}}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:qlora}{{2}{3}{Difference in finetuning methods. QLoRA supplements LoRA by applying quantization to the transformer model. \cite {dettmers2023qlora}}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Settings}{3}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of our application of LoRA adapters.}}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:lora_impl}{{3}{3}{Illustration of our application of LoRA adapters}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{3}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of our usage of QLoRA.}}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:qlora_impl}{{4}{4}{Illustration of our usage of QLoRA}{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of LoRA and QLoRA on Llama 2 (7B) Model.}}{4}{table.caption.5}\protected@file@percent }
\newlabel{tab:lora_qlora_comparison}{{1}{4}{Comparison of LoRA and QLoRA on Llama 2 (7B) Model}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of LoRA and QLoRA on Llama 3.2 (1B) Model.}}{5}{table.caption.6}\protected@file@percent }
\newlabel{tab:lora_qlora_3_2}{{2}{5}{Comparison of LoRA and QLoRA on Llama 3.2 (1B) Model}{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Leaderboard results from various team implementations for SemEval 2021 - Task 6, Subtask 1. The baseline performance is 0.06439. All of our implementations beat the baseline performance, besides the Llama 3.2 - 1B FFT implementation, which resulted in poor performance.}}{5}{figure.caption.7}\protected@file@percent }
\newlabel{fig:leaderboard}{{5}{5}{Leaderboard results from various team implementations for SemEval 2021 - Task 6, Subtask 1. The baseline performance is 0.06439. All of our implementations beat the baseline performance, besides the Llama 3.2 - 1B FFT implementation, which resulted in poor performance}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion and Notes}{5}{section.6}\protected@file@percent }
\citation{quamba}
\citation{li2024instruction}
\bibstyle{acl_natbib}
\bibdata{references}
\bibcite{quamba}{{1}{2024}{{Chiang et~al.}}{{Chiang, Chang, Frumkin, Wu, and Marculescu}}}
\bibcite{dettmers2023qlora}{{2}{2023}{{Dettmers et~al.}}{{Dettmers, Pagnoni, Holtzman, and Zettlemoyer}}}
\bibcite{he2022unified}{{3}{2022}{{He et~al.}}{{He, Zhou, Ma, Berg-Kirkpatrick, and Neubig}}}
\bibcite{hu2021lora}{{4}{2021}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}}}
\bibcite{li2024instruction}{{5}{2024}{{Li et~al.}}{{Li, Pang, and Zhao}}}
\bibcite{liu2024dora}{{6}{2024}{{Liu et~al.}}{{Liu, Wang, Yin, Molchanov, Wang, Cheng, and Chen}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{6}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Future Work}{6}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Contributions}{6}{section.9}\protected@file@percent }
\bibcite{novikova2017e2e}{{7}{2017}{{Novikova et~al.}}{{Novikova, Du{\v {s}}ek, and Rieser}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Quantization Implementation}{7}{appendix.A}\protected@file@percent }
\newlabel{sec:appendix}{{A}{7}{Quantization Implementation}{appendix.A}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{\ignorespaces NF4 Quantization Configuration}}{7}{lstlisting.1}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}{\ignorespaces Loading a Quantized Model}}{7}{lstlisting.2}\protected@file@percent }
\gdef \@abspage@last{7}
